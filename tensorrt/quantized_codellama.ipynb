{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: LicenseRef-NvidiaProprietary\n",
    "\n",
    "# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual\n",
    "# property and proprietary rights in and to this material, related\n",
    "# documentation and any modifications thereto. Any use, reproduction,\n",
    "# disclosure or distribution of this material and related documentation\n",
    "# without an express license agreement from NVIDIA CORPORATION or\n",
    "# its affiliates is strictly prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Co-pilot: CodeLlama 34B Go Brrr on A SINGLE 24 GB GPU!\n",
    "\n",
    "CodeLlama 34B Instruct, a conversational model is designed for general code synthesis and understanding. It is capable of code completion and instruct following.\n",
    "\n",
    "In this notebook, we will build a local Co-pilot using CodeLlama 34B Instruct which runs on a single GPU with 24 GB memory.\n",
    "The original CodeLlama 34B has 34B parameters and requires ~ 64 GB of GPU memory. It is not possible to fit the entire model in our 24 GB GPU and run fast inference &#x1F623;.\n",
    "\n",
    "## TensorRT-LLM To The Rescue !!\n",
    "\n",
    "Therefore, we will use **TensorRT/TensorRT-LLM Quantization Toolkit** to quantize the model weights to INT4 (W4A16) and build a TensorRT-LLM (TRTLLM) engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Please follow the installation steps [here](https://github.com/NVIDIA/TensorRT-LLM?tab=readme-ov-file#installation) to create TensorRT-LLM docker image.\n",
    "\n",
    "TensorRT-LLM quantization toolkit, named `nvidia-ammo` automatically gets installed with TensorRT-LLM.\n",
    "\n",
    "In this tutorial, we will be using the `nvidia-ammo 0.9`, the latest version of ammo which got released just last week. The 'nvidia-ammo' installed by default in the TensorRT-LLM image is an older version. So lets install the latest one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nvidia-ammo~=0.9 --extra-index-url https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional System Requirements\n",
    "\n",
    "This demo will be done in two stages.\n",
    "\n",
    "* **Stage 1: Pytorch Model Quantization & Checkpoint Export**: This stage is run on a machine with atleast 4 x 24GB GPUs\n",
    "\n",
    "* **Stage 2: TRTLLM Engine Build & Inference**: This stage is run on a machine with single 24 GB GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0: Importing Packages &  Defining Arguments (Common for both Stage 1 & Stage 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install the remaining required packages to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import all the required python packages first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "import time\n",
    "import torch\n",
    "import ammo.torch.quantization as atq\n",
    "import ammo.torch.opt as ato\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from ammo.torch.export import export_tensorrt_llm_checkpoint\n",
    "from ammo.deploy.llm import build_tensorrt_llm\n",
    "from ammo.deploy.llm import LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"CodeLlama/CodeLlama-34b-Instruct-hf\"\n",
    "\n",
    "# TRTLLM engine configurations\n",
    "model_type = \"llama\"\n",
    "export_path = \"CodeLlama-34b-Instruct-int4-awq\"\n",
    "tensor_parallel_world_size = 1\n",
    "max_input_len = 2048\n",
    "max_output_len = 512\n",
    "max_batch_size = 1\n",
    "\n",
    "engine_dir = Path(export_path, f\"{model_type}_input{max_input_len}_output{max_output_len}_batch{max_batch_size}_tp{tensor_parallel_world_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\", truncation_side=\"left\",use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a chat version of Codellama. The chat model input needs to be user queries empedded into the the following prompt: ` \"<s>[INST] {query} [/INST]\" `.\n",
    "Lets write a function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Write a python function to check if the input string is a palindrome or not. [/INST]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prompt(query):\n",
    "    return f\"<s>[INST] {query.strip()} [/INST]\"\n",
    "\n",
    "query = \"Write a python function to check if the input string is a palindrome or not.\"\n",
    "get_prompt(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Pytorch Model Quantization & Checkpoint Export\n",
    "\n",
    "Inference on the original pytorch fp16 model requires about 4 X 24 GPUs. Hence we will run this stage on a machine with atleast 4 X 24GB GPUs.\n",
    "\n",
    "Now, lets instantiate the original un-quantized pytorch CodeLlama model from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:18<00:00,  2.60s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the model size and how much memory it occupies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 63.23 GB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size(model):\n",
    "    size = sum(p.numel()*p.element_size() for p in model.parameters())\n",
    "    size += sum(b.numel()*b.element_size() for b in model.buffers())\n",
    "    return size\n",
    "print(f\"Model size: {get_model_size(model)/2**30:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now implement a text generator function on top of the Huggingface model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pytorch(\n",
    "    model, tokenizer, prompt, num_tokens_to_generate=512, do_sample=False\n",
    "    ):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=num_tokens_to_generate,\n",
    "            do_sample=do_sample,\n",
    "        )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True).split('[/INST]')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should test the model's generation capabilities now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Here is a Python function that checks if a given string is a palindrome or not:\n",
      "```\n",
      "def is_palindrome(s):\n",
      "    return s == s[::-1]\n",
      "```\n",
      "This function takes a string `s` as input and checks if it is equal to its reverse using the slicing notation `s[::-1]`. If the string is a palindrome, the function returns `True`, otherwise it returns `False`.\n",
      "\n",
      "Here's an example of how you can use this function:\n",
      "```\n",
      ">>> is_palindrome(\"racecar\")\n",
      "True\n",
      ">>> is_palindrome(\"not a palindrome\")\n",
      "False\n",
      "```\n",
      "This function works by taking advantage of the fact that a palindrome is a string that reads the same backward as it does forward. By comparing the original string to its reverse, we can determine if it is a palindrome or not.\n"
     ]
    }
   ],
   "source": [
    "query = \"Write a python function to check if the input string is a palindrome or not.\"\n",
    "response = generate_pytorch(model, tokenizer, get_prompt(query))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quantize this model now. First, lets create a dataloader for providing calibration data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading calibration dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 15.6k/15.6k [00:00<00:00, 20.9MB/s]\n",
      "Downloading data: 100%|██████████| 257M/257M [00:04<00:00, 54.6MB/s] \n",
      "Downloading data: 100%|██████████| 257M/257M [00:03<00:00, 66.1MB/s] \n",
      "Downloading data: 100%|██████████| 259M/259M [00:04<00:00, 62.0MB/s] \n",
      "Downloading data: 100%|██████████| 34.7M/34.7M [00:00<00:00, 48.0MB/s]\n",
      "Downloading data: 100%|██████████| 30.0M/30.0M [00:00<00:00, 53.3MB/s]\n",
      "Generating train split: 100%|██████████| 287113/287113 [00:03<00:00, 82949.88 examples/s]\n",
      "Generating validation split: 100%|██████████| 13368/13368 [00:00<00:00, 85873.74 examples/s]\n",
      "Generating test split: 100%|██████████| 11490/11490 [00:00<00:00, 78085.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def get_calib_dataloader(tokenizer=None, batch_size=1, calib_size=512, block_size=512, device=None\n",
    "):\n",
    "    print(\"Loading calibration dataset\")\n",
    "    dataset = load_dataset(\"cnn_dailymail\", name=\"3.0.0\", split=\"train\")\n",
    "    \n",
    "    #dataset = load_dataset(\"bigcode/the-stack\", data_dir=\"data/python\", split=\"val\")\n",
    "    \n",
    "    dataset = dataset[\"article\"][:calib_size]\n",
    "\n",
    "    batch_encoded = tokenizer.batch_encode_plus(\n",
    "        dataset, return_tensors=\"pt\", padding=True, truncation=True, max_length=block_size\n",
    "    )\n",
    "    if device:\n",
    "        batch_encoded = batch_encoded.to(device)\n",
    "    batch_encoded = batch_encoded[\"input_ids\"]\n",
    "\n",
    "    calib_dataloader = DataLoader(batch_encoded, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return calib_dataloader\n",
    "\n",
    "calib_dataloader = get_calib_dataloader(\n",
    "                tokenizer=tokenizer,\n",
    "                batch_size=32,\n",
    "                calib_size=32,\n",
    "                device=torch.device(\"cuda\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything we need for quantizing and calibrating the model now. AMMO's `atq.quantize()` API is used for performing quantization and calibration. \n",
    "\n",
    "The model to be quantized, the quantization configuration and a `forward_loop` which forwards the calibration data through the model are the inputs to `atq.quantize()`.\n",
    "\n",
    "This API inserts simulated quantizer/dequantizer (Q/DQ) nodes to the model, calls `forward_loop` and collect the needed statistics for quantization calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 1011 modules to quantized modules\n",
      "Caching activation statistics for awq_lite...\n",
      "Calibrating model...\n",
      "Batch: 1/1\n",
      "Searching awq_lite parameters...\n",
      "Calibrating model...\n",
      "Batch: 1/1\n",
      "Loading extension ammo_cuda_ext...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ammo/torch/quantization/nn/modules/tensor_quantizer.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"_pre_quant_scale\", torch.tensor(value))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading extension ammo_cuda_ext_fp8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ammo/torch/quantization/nn/modules/tensor_quantizer.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(value, device=self._pre_quant_scale.device)\n",
      "/usr/local/lib/python3.10/dist-packages/ammo/torch/quantization/nn/modules/tensor_quantizer.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"_pre_quant_scale\", torch.tensor(value))\n"
     ]
    }
   ],
   "source": [
    "def forward_loop(model):\n",
    "        print(\"Calibrating model...\")\n",
    "        for i, data in enumerate(calib_dataloader):\n",
    "            print(f\"Batch: {i+1}/{len(calib_dataloader)}\")\n",
    "            model(data)\n",
    "\n",
    "# Calibration loop\n",
    "quantized_model = atq.quantize(model, atq.INT4_AWQ_CFG, forward_loop)\n",
    "\n",
    "# To save the model for future use:\n",
    "# ato.save(quantized_model, \"quantized_int4_model.pt\")\n",
    "\n",
    "# To restore the model from saved checkpoint:\n",
    "# Give the original, unquantized model as input to the restore function\n",
    "# quantized_model = ato.restore(model, \"quantized_int4_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print the quantized model summary and make sure that the quantizer nodes are placed correctly in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0028, 7.4648](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0092, 2.9199](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0086, 0.4426](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0066, 1.4648](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0105, 1.1426](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0094, 0.5801](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0076, 8.2734](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0167, 4.2070](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0235, 8.4844](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0051, 0.2372](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0080, 1.4521](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0161, 0.7217](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0124, 0.6606](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0048, 5.4805](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0186, 1.0088](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0177, 1.5830](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0097, 0.2632](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0123, 2.3770](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0174, 0.5732](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0112, 0.9590](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0043, 5.8906](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0185, 1.3945](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0179, 1.4355](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0075, 0.0986](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0160, 0.9629](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0085, 0.9541](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0100, 1.3906](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0017, 27.6875](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0119, 1.0010](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0141, 1.3232](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0082, 0.1192](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0151, 1.5889](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0110, 0.7153](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0103, 0.8892](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0021, 18.9688](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0069, 0.6108](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0170, 0.8271](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0075, 0.1416](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0144, 1.0488](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0268, 0.8950](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0091, 0.7754](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0109, 1.4014](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0047, 0.2461](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0154, 0.9702](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0089, 0.1228](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0151, 0.8760](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0230, 0.7681](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0118, 0.7881](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0130, 1.0566](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0081, 0.5469](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0167, 1.0010](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0067, 0.0944](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0156, 0.5327](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0280, 1.0264](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0163, 0.7710](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0137, 1.5889](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0087, 0.5347](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0160, 0.7622](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0083, 0.0847](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0143, 1.0049](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0319, 1.1240](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0178, 0.9092](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0153, 0.7422](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0066, 0.5083](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0140, 0.8965](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0078, 0.0659](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0135, 0.9390](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0174, 0.7036](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0136, 0.7397](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0137, 1.3350](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0097, 0.3665](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0208, 1.0068](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0068, 0.0931](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0126, 0.5503](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0173, 1.1318](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0169, 0.8057](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0125, 1.1885](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0106, 0.3938](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0173, 0.8760](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0074, 0.0800](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0135, 0.4185](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0241, 0.6279](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0200, 0.7051](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0092, 3.5918](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0090, 0.3499](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0195, 1.1182](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0062, 0.0729](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0156, 0.6030](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0247, 1.1650](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0213, 0.4880](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0176, 1.2734](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0111, 0.3801](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0204, 0.9854](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0067, 0.0703](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0163, 0.8555](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0224, 0.5825](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0191, 0.4756](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0158, 0.9082](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0088, 0.3372](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0214, 1.0625](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0055, 0.0811](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0125, 0.4622](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0229, 0.8672](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0179, 1.1035](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0141, 3.0078](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0077, 0.3347](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0236, 0.9941](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0061, 0.0862](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0164, 0.7207](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0206, 0.5249](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0176, 0.4338](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0091, 2.2871](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0103, 0.3682](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0201, 1.0830](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0077, 0.1489](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0174, 0.7002](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0204, 0.5820](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0141, 1.2188](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0145, 1.3906](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0103, 0.3362](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0266, 1.0752](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0068, 0.1309](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0142, 1.0850](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0215, 0.9111](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0219, 0.4817](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0140, 1.2256](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0098, 0.3215](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0179, 0.7422](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0066, 0.1077](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0155, 0.6699](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0258, 1.2070](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0229, 0.8452](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0119, 2.8965](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0086, 0.3677](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0180, 1.0840](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0092, 0.1464](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0182, 0.7002](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0191, 0.6729](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0092, 1.2725](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0094, 3.6953](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0098, 0.3269](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0169, 0.9326](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0080, 0.0958](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0129, 1.6172](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0296, 1.0146](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0323, 0.7524](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0121, 1.2988](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0091, 0.3123](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0175, 0.8052](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0081, 0.1046](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0126, 1.0527](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0309, 0.8843](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0318, 0.9429](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0112, 2.5371](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0098, 0.3481](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0168, 0.7959](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0087, 0.1294](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0144, 0.5459](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0262, 0.7651](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0248, 0.7793](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0104, 3.1543](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0092, 0.5479](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0162, 0.8857](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0096, 0.1400](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0134, 2.5879](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0246, 0.4934](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0253, 0.6094](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0130, 2.2578](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0095, 0.3398](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0208, 0.8887](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0107, 0.1362](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0177, 0.7896](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0383, 0.7979](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0337, 0.7485](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0112, 2.8691](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0089, 0.3171](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0135, 0.4370](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0103, 0.1072](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0147, 0.5078](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0415, 1.2090](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0394, 0.8770](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0139, 1.6777](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0092, 0.5522](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0202, 0.7812](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0115, 0.1071](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0153, 1.3770](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0167, 1.1113](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0208, 1.3691](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0129, 1.1240](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0073, 0.2625](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0198, 0.7871](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0094, 0.1141](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0185, 1.8301](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0252, 0.5244](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0249, 0.5781](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0143, 1.3896](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.28.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.28.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.28.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0075, 0.2449](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.28.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.28.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.28.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0149, 0.6274](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.28.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.28.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.28.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0089, 0.1032](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.28.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.28.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.28.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0172, 0.7573](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.28.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.28.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.28.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0219, 0.5396](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.28.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.28.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.28.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0207, 0.5166](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.28.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.28.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.28.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0087, 4.4805](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.29.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.29.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.29.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0086, 0.2979](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.29.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.29.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.29.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0197, 0.7588](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.29.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.29.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.29.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0104, 0.1394](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.29.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.29.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.29.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0144, 1.2988](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.29.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.29.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.29.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0176, 0.7002](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.29.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.29.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.29.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0217, 0.6382](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.29.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.29.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.29.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0120, 1.5107](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.30.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.30.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.30.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0089, 0.8916](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.30.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.30.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.30.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0164, 0.4065](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.30.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.30.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.30.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0166, 0.1823](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.30.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.30.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.30.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0151, 2.2949](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.30.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.30.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.30.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0442, 0.8779](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.30.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.30.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.30.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0351, 0.9517](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.30.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.30.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.30.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0098, 2.2832](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.31.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.31.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.31.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0076, 0.2186](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.31.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.31.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.31.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0121, 0.3706](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.31.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.31.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.31.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0094, 0.1178](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.31.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.31.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.31.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0161, 0.6953](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.31.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.31.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.31.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0327, 0.6572](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.31.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.31.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.31.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0281, 0.5469](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.31.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.31.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.31.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0107, 2.1602](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.32.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.32.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.32.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0073, 0.3047](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.32.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.32.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.32.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0159, 0.4756](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.32.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.32.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.32.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0092, 0.1476](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.32.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.32.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.32.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0169, 0.8101](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.32.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.32.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.32.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0351, 0.7979](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.32.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.32.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.32.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0236, 0.6948](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.32.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.32.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.32.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0075, 3.9824](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.33.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.33.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.33.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0072, 0.2052](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.33.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.33.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.33.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0168, 0.6201](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.33.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.33.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.33.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0114, 0.1241](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.33.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.33.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.33.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0177, 1.5000](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.33.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.33.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.33.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0659, 1.1650](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.33.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.33.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.33.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0523, 0.7729](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.33.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.33.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.33.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0129, 1.5332](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.34.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.34.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.34.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0082, 0.2502](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.34.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.34.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.34.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0151, 0.6108](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.34.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.34.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.34.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0104, 0.1237](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.34.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.34.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.34.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0106, 1.2021](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.34.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.34.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.34.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0331, 0.6353](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.34.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.34.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.34.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0266, 0.5273](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.34.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.34.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.34.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0131, 1.1045](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.35.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.35.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.35.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0082, 1.1855](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.35.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.35.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.35.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0127, 0.5142](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.35.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.35.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.35.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0117, 0.1774](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.35.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.35.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.35.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0162, 2.4785](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.35.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.35.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.35.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0348, 1.0957](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.35.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.35.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.35.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0285, 0.9214](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.35.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.35.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.35.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0065, 3.2324](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.36.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.36.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.36.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0085, 0.2629](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.36.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.36.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.36.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0162, 0.5659](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.36.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.36.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.36.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0114, 0.1620](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.36.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.36.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.36.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0145, 0.7280](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.36.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.36.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.36.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0320, 1.0537](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.36.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.36.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.36.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0267, 0.5117](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.36.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.36.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.36.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0115, 1.3535](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.37.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.37.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.37.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0092, 0.2717](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.37.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.37.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.37.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0188, 0.4031](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.37.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.37.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.37.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0130, 0.1918](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.37.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.37.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.37.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0136, 1.6797](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.37.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.37.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.37.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0275, 0.6357](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.37.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.37.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.37.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0207, 1.4590](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.37.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.37.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.37.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0075, 3.3066](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.38.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.38.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.38.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0090, 0.2795](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.38.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.38.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.38.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0191, 0.6509](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.38.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.38.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.38.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0124, 0.1422](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.38.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.38.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.38.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0154, 1.1416](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.38.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.38.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.38.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0312, 0.7349](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.38.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.38.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.38.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0261, 0.4497](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.38.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.38.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.38.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0116, 0.5625](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.39.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.39.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.39.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0087, 0.2925](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.39.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.39.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.39.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0194, 0.6416](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.39.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.39.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.39.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0120, 0.1259](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.39.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.39.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.39.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0131, 1.5625](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.39.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.39.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.39.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0267, 0.7964](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.39.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.39.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.39.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0221, 0.8296](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.39.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.39.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.39.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0137, 0.9380](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.40.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.40.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.40.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0058, 0.2284](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.40.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.40.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.40.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0149, 0.4426](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.40.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.40.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.40.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0111, 0.1359](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.40.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.40.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.40.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0160, 2.0508](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.40.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.40.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.40.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0247, 0.4746](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.40.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.40.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.40.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0211, 0.4153](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.40.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.40.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.40.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0130, 1.1367](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.41.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.41.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.41.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0106, 0.3035](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.41.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.41.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.41.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0211, 0.3821](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.41.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.41.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.41.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0101, 0.1324](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.41.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.41.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.41.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0113, 1.1465](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.41.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.41.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.41.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0224, 0.5391](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.41.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.41.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.41.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0190, 0.4221](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.41.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.41.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.41.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0114, 2.0117](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.42.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.42.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.42.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0066, 0.2404](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.42.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.42.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.42.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0167, 0.3135](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.42.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.42.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.42.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0122, 0.1626](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.42.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.42.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.42.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0132, 0.8213](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.42.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.42.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.42.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0195, 0.5986](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.42.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.42.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.42.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0191, 0.3901](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.42.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.42.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.42.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0091, 2.6465](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.43.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.43.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.43.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0057, 0.2484](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.43.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.43.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.43.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0155, 0.4277](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.43.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.43.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.43.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0117, 0.0874](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.43.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.43.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.43.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0149, 1.0000](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.43.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.43.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.43.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0217, 0.3669](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.43.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.43.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.43.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0207, 0.3660](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.43.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.43.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.43.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0138, 1.7695](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.44.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.44.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.44.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0057, 0.1781](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.44.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.44.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.44.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0133, 0.3318](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.44.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.44.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.44.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0129, 0.1639](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.44.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.44.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.44.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0111, 1.1348](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.44.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.44.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.44.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0180, 0.7588](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.44.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.44.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.44.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0160, 0.5474](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.44.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.44.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.44.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0128, 1.5508](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.45.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.45.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.45.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0056, 0.2478](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.45.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.45.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.45.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0139, 0.3037](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.45.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.45.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.45.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0132, 0.1689](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.45.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.45.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.45.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0135, 0.7598](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.45.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.45.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.45.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0193, 0.8149](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.45.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.45.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.45.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0169, 0.6245](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.45.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.45.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.45.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0134, 2.6758](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.46.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.46.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.46.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0063, 0.1794](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.46.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.46.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.46.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0124, 0.2805](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.46.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.46.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.46.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0098, 0.1084](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.46.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.46.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.46.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0181, 1.6768](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.46.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.46.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.46.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0177, 1.0352](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.46.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.46.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.46.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0151, 0.7759](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.46.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.46.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.46.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0055, 5.8164](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.47.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.47.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.47.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0054, 0.5083](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.47.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.47.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.47.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0101, 0.5605](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.47.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.47.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.47.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0088, 0.0870](65536) calibrator=MaxCalibrator quant)\n",
      "model.layers.47.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.47.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.47.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0129, 1.5303](524288) calibrator=MaxCalibrator quant)\n",
      "model.layers.47.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.47.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.47.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0137, 0.5020](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.47.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.47.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.47.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0079, 0.8042](1409024) calibrator=MaxCalibrator quant)\n",
      "model.layers.47.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.47.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.47.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128}, amax=[0.0083, 5.2148](1409024) calibrator=MaxCalibrator quant)\n",
      "lm_head.input_quantizer                                                          TensorQuantizer(disabled)\n",
      "lm_head.output_quantizer                                                         TensorQuantizer(disabled)\n",
      "lm_head.weight_quantizer                                                         TensorQuantizer(disabled)\n",
      "1011 TensorQuantizers found in model\n"
     ]
    }
   ],
   "source": [
    "atq.print_quant_summary(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantized model is a fully valid pytorch model. You can evaluate the quantized model same as you evaluate a regular Pytorch model. Lets check the text generated by the model and make sure everything works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Here is a Python function that checks if a given string is a palindrome or not:\n",
      "```\n",
      "def is_palindrome(s):\n",
      "    return s == s[::-1]\n",
      "```\n",
      "Explanation:\n",
      "\n",
      "* The function takes a string `s` as input.\n",
      "* It checks if the string is equal to its reverse using the slicing notation `s[::-1]`. This notation returns the string in reverse order.\n",
      "* If the string is equal to its reverse, it means that it is a palindrome, and the function returns `True`.\n",
      "* If the string is not equal to its reverse, it means that it is not a palindrome, and the function returns `False`.\n",
      "\n",
      "Here's an example of how to use the function:\n",
      "```\n",
      "print(is_palindrome(\"racecar\")) # Output: True\n",
      "print(is_palindrome(\"not a palindrome\")) # Output: False\n",
      "```\n",
      "Note that this function assumes that the input string is not empty. If the input string is empty, the function will return `True` because an empty string is considered a palindrome. If you want to handle empty strings differently, you can modify the function accordingly.\n"
     ]
    }
   ],
   "source": [
    "query = \"Write a python function to check if the input string is a palindrome or not.\"\n",
    "response = generate_pytorch(quantized_model, tokenizer, get_prompt(query))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets export the model checkpoint. This checkpoint will be used by TRTLLM to build the engine.\n",
    "Please see [prepare-the-tensorrt-llm-checkpoint](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/new_workflow.md#prepare-the-tensorrt-llm-checkpoint) to learn more about the checkpoint format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.distributed not initialized, assuming single world_size.\n",
      "torch.distributed not initialized, assuming single world_size.\n",
      "torch.distributed not initialized, assuming single world_size.\n",
      "torch.distributed not initialized, assuming single world_size.\n",
      "torch.distributed not initialized, assuming single world_size.\n",
      "torch.distributed not initialized, assuming single world_size.\n",
      "torch.distributed not initialized, assuming single world_size.\n",
      "torch.distributed not initialized, assuming single world_size.\n",
      "current rank: 0, tp rank: 0, pp rank: 0\n",
      "torch.distributed not initialized, assuming single world_size.\n",
      "Quantized model exported to : CodeLlama-34b-Instruct-int4-awq. Total time used 149.681 s\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    start_time = time.time()\n",
    "    export_tensorrt_llm_checkpoint(\n",
    "        quantized_model,\n",
    "        model_type,\n",
    "        torch.float16,\n",
    "        export_dir=export_path,\n",
    "        inference_tensor_parallel=tensor_parallel_world_size, # you may modify this number to enable multi GPU TP\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(\n",
    "        f\"Quantized model exported to : {export_path}. Total time used {end_time - start_time:.3f} s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model checkpoint has been exported. **Stage 1: Pytorch Model Quantization & Checkpoint Export** is complete. Lets stop execution on the multi-GPU machine now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: TRTLLM Engine Build & Inference\n",
    "\n",
    "We are ready to build TRTLLM engine now. The engine is built on the same machine it needs to be served which is a machine with single 24 GB GPU for this demo. So this stage is ran from the machine with single 24GB GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build the TRTLLM engine now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/17/2024-15:41:28] [TRT] [W] Unused Input: position_ids\n",
      "[03/17/2024-15:41:29] [TRT] [W] Detected layernorm nodes in FP16.\n",
      "[03/17/2024-15:41:29] [TRT] [W] Running layernorm after self-attention in FP16 may cause overflow. Exporting the model to the latest available ONNX opset (later than opset 17) to use the INormalizationLayer, or forcing layernorm layers to run in FP32 precision can help with preserving accuracy.\n",
      "[03/17/2024-15:41:29] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[03/17/2024-15:41:29] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4\n",
      "[03/17/2024-15:42:16] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4\n"
     ]
    }
   ],
   "source": [
    "config_path = Path(export_path, f\"config.json\")\n",
    "\n",
    "build_tensorrt_llm(\n",
    "        pretrained_config=config_path,\n",
    "        engine_dir=engine_dir,\n",
    "        max_input_len=max_input_len,\n",
    "        max_output_len=max_output_len,\n",
    "        max_batch_size=max_batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it! TRTLLM engine have been built. Lets load the engine and run inference using TRTLLM python runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Model: \u001b[1;32m[1/1]\t\u001b[0mLoad TensorRT-LLM engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/17/2024-15:44:57] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4\n",
      "[03/17/2024-15:44:58] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4\n",
      "[03/17/2024-15:44:58] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4\n",
      "[03/17/2024-15:44:58] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20mTime: 92.764s\n",
      "\u001b[0m\u001b[1;32mLoading model done.\n",
      "\u001b[0m\u001b[38;20mTotal latency: 92.764s\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "free_memory_before = torch.cuda.mem_get_info()\n",
    "llm = LLM(engine_dir, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nested/__init__.py:165: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  return _nested.nested_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a Python function that checks if a given string is a palindrome or not:\n",
      "```\n",
      "def is_palindrome(s):\n",
      "    return s == s[::-1]\n",
      "```\n",
      "Explanation:\n",
      "\n",
      "* The function takes a string `s` as input.\n",
      "* It checks if the string is equal to its reverse using the slicing notation `s[::-1]`. This notation returns the string in reverse order.\n",
      "* If the string is equal to its reverse, it means that it is a palindrome, and the function returns `True`.\n",
      "* If the string is not equal to its reverse, it means that it is not a palindrome, and the function returns `False`.\n",
      "\n",
      "Here's an example of how you can use this function:\n",
      "```\n",
      "print(is_palindrome(\"racecar\")) # Output: True\n",
      "print(is_palindrome(\"not a palindrome\")) # Output: False\n",
      "```\n",
      "Note that this function is case-sensitive, so \"Racecar\" and \"racecar\" are considered different strings. If you want to make the function case-insensitive, you can convert the input string to lowercase before comparing it to its reverse.\n"
     ]
    }
   ],
   "source": [
    "query = \"Write a python function to check if the input string is a palindrome or not.\"\n",
    "outputs = llm.generate_text([get_prompt(query)], max_output_len)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me ask our Co-pilot to help me on something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a possible implementation of the method you described:\n",
      "```\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def plot_human_eval_scores(scores):\n",
      "    # Create a bar chart of the HumanEval scores\n",
      "    plt.bar(scores.keys(), scores.values(), color=['darkseagreen', 'lawngreen'])\n",
      "    plt.grid(True, axis='y')\n",
      "    plt.title('HumanEval Scores of Codellama models')\n",
      "    plt.ylim(min(scores.values()) - 5, max(scores.values()) + 5)\n",
      "    plt.xlabel('Model Name')\n",
      "    plt.ylabel('HumanEval Score')\n",
      "    plt.show()\n",
      "```\n",
      "You can call this method by passing in the dictionary of HumanEval scores as an argument, like this:\n",
      "```\n",
      "scores = {'Codellama 7b FP16': 33.64, 'Codellama 34b W4A16': 43.6}\n",
      "plot_human_eval_scores(scores)\n",
      "```\n",
      "This will create a bar chart with two bars, one for each model, and display it using Matplotlib. The first bar will be colored 'darkseagreen' and the second bar will be colored 'lawngreen'. The y-axis grid lines will be turned on, and the title of the plot will be 'HumanEval Scores of Codellama models'. The y-axis range will be set to (minimum HumanEval score - 5, maximum HumanEval score + 5), and the x-axis label will be 'Model Name'. The y-axis label will be 'HumanEval Score'.\n"
     ]
    }
   ],
   "source": [
    "human_eval_scores = {\"Codellama 7b FP16\": 33.64, \"Codellama 34b W4A16\": 43.60}\n",
    "query= \"Here is a dictionary where the keys are Codellama model names \" \\\n",
    "    f\"and the values are the HumanEval scores of the model: {human_eval_scores}. \" \\\n",
    "    \"Write a Python method to plot a bar chart of the HumanEval scores. \" \\\n",
    "    \"Give the first bar the color 'darkseagreen' and the second bar the color 'lawngreen'.\" \\\n",
    "    \"Turn on the y-axis grid lines and set the title of the plot to 'HumanEval Scores of Codellama models'. \" \\\n",
    "    \"The y-axis range should be (minimum HumanEval score - 5, maximum HumanEval score + 5). \" \\\n",
    "    \"The x-axis label should be 'Model Name' and the y-axis label should be 'HumanEval Score'.\"\n",
    "outputs = llm.generate_text([get_prompt(query)], max_output_len)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you Local Co-Pilot for the code!! &#x1F600; Productivity is off to the moon &#128640;\n",
    "\n",
    "Lets use the Co-pilot generated code to make the accuracy plot for CodeLlama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_human_eval_scores(scores):\n",
    "    # Create a bar chart of the HumanEval scores\n",
    "    plt.bar(scores.keys(), scores.values(), color=['darkseagreen', 'lawngreen'])\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.title('HumanEval Scores of Codellama models')\n",
    "    plt.ylim(min(scores.values()) - 5, max(scores.values()) + 5)\n",
    "    plt.xlabel('Model Name')\n",
    "    plt.ylabel('HumanEval Score')\n",
    "    plt.show()\n",
    "\n",
    "scores = {'Codellama 7b FP16': 33.64, 'Codellama 34b W4A16': 43.6}\n",
    "plot_human_eval_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Takeaway:\n",
    "\n",
    "The original CodeLlama 34B needed more than 63 GB of memory. We have compressed the model memory to 17 GB from 63 GB **enabling inference of CodaLlama 34B on a single 24GB GPU!**.\n",
    "\n",
    "Moreover, this TRTLLM quantized CodeLlama 34B model's accuracy is much better than that of CodeLlama 7B (which is about 14 GB in size). Both of these models require similar resources to run. \n",
    "\n",
    "**Best in class models are very often achieved by optimizing a large model down to meet your requirements rather than starting with a smaller model.**\n",
    "\n",
    "We have several other effective optimization techniques such as NAS, pruning, sparsity etc. We are releasing them as preview features for GTC'2024. Please reach out to us if you would like to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets stop the profiler now and check how much memory our code generation consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used GPU memory: 17.736328125 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.cudart().cudaProfilerStop()\n",
    "free_memory_after = torch.cuda.mem_get_info()\n",
    "print(\n",
    "    f\"Used GPU memory: {(free_memory_before[0] - free_memory_after[0]) / 1024 / 1024 / 1024} GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have successfully quantized CodeLlama 34B Pytorch model and built TRTLLM engine for fast and cost-effective inference. \n",
    "The quantized CodeLlama 34B is significantly better than models requiring similar compute-resources in coding capabilities.\n",
    "\n",
    "Quantization is thus a very effective model optimization technique which compresses the model, accelerate inference while retaining most of the model quality.\n",
    "\n",
    "This is almost a free lunch !!! &#128571; &#128571;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
