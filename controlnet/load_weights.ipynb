{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Memory Usage:\n",
      "  Total Memory: 62.55 GB\n",
      "  Used Memory: 9.94 GB\n",
      "  Free Memory: 51.51 GB\n",
      "GPU Memory Usage:\n",
      "  Total Memory: 14.58 GB\n",
      "  Allocated Memory: 6.57 GB\n",
      "  Reserved Memory: 6.84 GB\n",
      "GPU 0 Memory Usage:\n",
      "  Total Memory: 14.58 GB\n",
      "  Free Memory: 7.64 GB\n",
      "  Used Memory: 6.94 GB\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, AutoencoderKL, UNet2DConditionModel\n",
    "from transformers import CLIPTokenizer, CLIPTextModel, CLIPTextModelWithProjection\n",
    "import torch\n",
    "from safetensors.torch import load_file, save_file\n",
    "from safetensors import safe_open\n",
    "from time import perf_counter\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def preload_safetensors_to_cpu(safetensors_path):\n",
    "    # 使用 safetensors 库读取权重到 CPU 内存\n",
    "    weights = load_file(safetensors_path, device=\"cpu\")\n",
    "    return weights\n",
    "\n",
    "def get_device_memory_usage(device=0):\n",
    "    # 获取 GPU 剩余显存信息\n",
    "    free_memory, total_memory = torch.cuda.mem_get_info(device)\n",
    "    free_memory_gb = free_memory / (1024 ** 3)  # 转换为 GB\n",
    "    total_memory_gb = total_memory / (1024 ** 3)  # 转换为 GB\n",
    "    used_memory_gb = total_memory_gb - free_memory_gb\n",
    "\n",
    "    print(f\"GPU {device} Memory Usage:\")\n",
    "    print(f\"  Total Memory: {total_memory_gb:.2f} GB\")\n",
    "    print(f\"  Free Memory: {free_memory_gb:.2f} GB\")\n",
    "    print(f\"  Used Memory: {used_memory_gb:.2f} GB\")\n",
    "\n",
    "def get_cpu_memory_usage():\n",
    "    # 获取 CPU 内存使用情况\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    cpu_memory_used = memory_info.used / (1024 ** 3)  # 转换为 GB\n",
    "    cpu_memory_total = memory_info.total / (1024 ** 3)  # 转换为 GB\n",
    "    cpu_memory_free = memory_info.available / (1024 ** 3)  # 转换为 GB\n",
    "\n",
    "    print(\"CPU Memory Usage:\")\n",
    "    print(f\"  Total Memory: {cpu_memory_total:.2f} GB\")\n",
    "    print(f\"  Used Memory: {cpu_memory_used:.2f} GB\")\n",
    "    print(f\"  Free Memory: {cpu_memory_free:.2f} GB\")\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    # 获取 GPU 显存使用情况\n",
    "    gpu_memory_allocated = torch.cuda.memory_allocated() / (1024 ** 3)  # 转换为 GB\n",
    "    gpu_memory_reserved = torch.cuda.memory_reserved() / (1024 ** 3)  # 转换为 GB\n",
    "    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # 转换为 GB\n",
    "\n",
    "    print(\"GPU Memory Usage:\")\n",
    "    print(f\"  Total Memory: {gpu_memory_total:.2f} GB\")\n",
    "    print(f\"  Allocated Memory: {gpu_memory_allocated:.2f} GB\")\n",
    "    print(f\"  Reserved Memory: {gpu_memory_reserved:.2f} GB\")\n",
    "\n",
    "# Example usage\n",
    "get_cpu_memory_usage()\n",
    "get_gpu_memory_usage()\n",
    "get_device_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# 清理 GPU 内存缓存\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".safetensors 方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet\n",
      "vae\n",
      "pipe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 45.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载一个 SD-XL 基座模型 所需时间为:  61.070760504342616 s\n",
      "CPU Memory Usage:\n",
      "  Total Memory: 62.55 GB\n",
      "  Used Memory: 9.47 GB\n",
      "  Free Memory: 51.98 GB\n",
      "GPU Memory Usage:\n",
      "  Total Memory: 14.58 GB\n",
      "  Allocated Memory: 6.57 GB\n",
      "  Reserved Memory: 6.84 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载基座\n",
    "start = perf_counter()\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "  '/data1/workspace/javeyqiu/models/stable-diffusion-xl-base-1.0',\n",
    "  subfolder='text_encoder',\n",
    "  use_safetensors=True,\n",
    "  torch_dtype=torch.float16,\n",
    "  variant='fp16',\n",
    ").to('cuda')\n",
    "text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(\n",
    "  '/data1/workspace/javeyqiu/models/stable-diffusion-xl-base-1.0',\n",
    "  subfolder='text_encoder_2',\n",
    "  use_safetensors=True,\n",
    "  torch_dtype=torch.float16,\n",
    "  variant='fp16',\n",
    ").to('cuda')\n",
    "print('unet')\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"/data1/workspace/javeyqiu/models/stable-diffusion-xl-base-1.0\",\n",
    "    subfolder='unet',\n",
    "    torch_dtype=torch.float16, \n",
    "    use_safetensors=True, \n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "print('vae')\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"madebyollin/sdxl-vae-fp16-fix\", \n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir='/data1/workspace/javeyqiu/models/huggingface/hub'\n",
    ").to(\"cuda\")\n",
    "print('pipe')\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"/data1/workspace/javeyqiu/models/stable-diffusion-xl-base-1.0\", \n",
    "    unet=unet,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    text_encoder_2=text_encoder_2,\n",
    "    use_safetensors=True, \n",
    "    torch_dtype=torch.float16, \n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "endtime = perf_counter()-start\n",
    "print(\"加载一个 SD-XL 基座模型 所需时间为: \", endtime, \"s\")\n",
    "get_cpu_memory_usage()\n",
    "get_gpu_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".bin 加载方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae, './stable-diffusion-xl-base-1.0/vae.bin')\n",
    "torch.save(unet, './stable-diffusion-xl-base-1.0/unet.bin')\n",
    "torch.save(text_encoder, './stable-diffusion-xl-base-1.0/text_encoder.bin')\n",
    "torch.save(text_encoder_2, './stable-diffusion-xl-base-1.0/text_encoder_2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 38.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载一个 SD-XL 基座模型 所需时间为:  2.7703366917558014 s\n",
      "Max memory allocated: 6.5670599937438965 GB\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "text_encoder = torch.load('./stable-diffusion-xl-base-1.0/text_encoder.bin').to(\"cuda\")\n",
    "text_encoder_2 = torch.load('./stable-diffusion-xl-base-1.0/text_encoder_2.bin').to(\"cuda\")\n",
    "unet = torch.load('./stable-diffusion-xl-base-1.0/unet.bin').to(\"cuda\")\n",
    "vae = torch.load('./stable-diffusion-xl-base-1.0/vae.bin').to(\"cuda\")\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"/data1/workspace/javeyqiu/models/stable-diffusion-xl-base-1.0\", \n",
    "    unet=unet,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    text_encoder_2=text_encoder_2,\n",
    "    use_safetensors=True, \n",
    "    torch_dtype=torch.float16, \n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "endtime = perf_counter()-start\n",
    "print(\"加载一个 SD-XL 基座模型 所需时间为: \", endtime, \"s\")\n",
    "get_cpu_memory_usage()\n",
    "get_gpu_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero Copy 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import copy\n",
    "def extract_tensors(m: torch.nn.Module) -> Tuple[torch.nn.Module, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Remove the tensors from a PyTorch model, convert them to NumPy\n",
    "    arrays, and return the stripped model and tensors.\n",
    "    \"\"\"\n",
    "    tensors = []\n",
    "    for _, module in m.named_modules():\n",
    "        # Store the tensors in Python dictionaries\n",
    "        params = {\n",
    "            name: torch.clone(param).detach()\n",
    "            for name, param in module.named_parameters(recurse=False)\n",
    "        }\n",
    "        buffers = {\n",
    "            name: torch.clone(buf).detach()\n",
    "            for name, buf in module.named_buffers(recurse=False)\n",
    "        }\n",
    "        tensors.append({\"params\": params, \"buffers\": buffers})\n",
    "    \n",
    "    # Make a copy of the original model and strip all tensors and\n",
    "    # buffers out of the copy.\n",
    "    m_copy = copy.deepcopy(m)\n",
    "    for _, module in m_copy.named_modules():\n",
    "        for name in ([name for name, _ in module.named_parameters(recurse=False)]\n",
    "                     + [name for name, _ in module.named_buffers(recurse=False)]):\n",
    "            setattr(module, name, None)   \n",
    "\n",
    "    # Make sure the copy is configured for inference.\n",
    "    m_copy.eval()\n",
    "    return m_copy, tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所需时间为:  1.1928686192259192 s\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "text_encoder_copy, text_encoder_tensors = extract_tensors(text_encoder.cpu())\n",
    "text_encoder_2_copy, text_encoder_2_tensors = extract_tensors(text_encoder_2.cpu())\n",
    "unet_copy, unet_tensors = extract_tensors(unet.cpu())\n",
    "vae_copy, vae_tensors = extract_tensors(vae.cpu())\n",
    "endtime = perf_counter()-start\n",
    "print(\"所需时间为: \", endtime, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_encoder_tensors, 'text_encoder_tensors.bin')\n",
    "torch.save(text_encoder_2_tensors, 'text_encoder_2_tensors.bin')\n",
    "torch.save(unet_tensors, 'unet_tensors.bin')\n",
    "torch.save(vae_tensors, 'vae_tensors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_tensors = torch.load('./stable-diffusion-xl-base-1.0/text_encoder_tensors.bin')\n",
    "text_encoder_2_tensors = torch.load('./stable-diffusion-xl-base-1.0/text_encoder_2_tensors.bin')\n",
    "unet_tensors = torch.load('./stable-diffusion-xl-base-1.0/unet_tensors.bin')\n",
    "vae_tensors = torch.load('./stable-diffusion-xl-base-1.0/vae_tensors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所需时间为:  0.8135069417767227 s\n"
     ]
    }
   ],
   "source": [
    "def replace_tensors(m: torch.nn.Module, tensors: List[Dict]):\n",
    "    \"\"\"\n",
    "    Restore the tensors that extract_tensors() stripped out of a \n",
    "    PyTorch model.\n",
    "    :param no_parameters_objects: Skip wrapping tensors in \n",
    "     ``torch.nn.Parameters`` objects (~20% speedup, may impact\n",
    "     some models)\n",
    "    \"\"\"\n",
    "    modules = [module for _, module in m.named_modules()] \n",
    "    for module, tensor_dict in zip(modules, tensors):\n",
    "        # There are separate APIs to set parameters and buffers.\n",
    "        for name, array in tensor_dict[\"params\"].items():\n",
    "            module.register_parameter(name, \n",
    "                torch.nn.Parameter(torch.as_tensor(array)))\n",
    "        for name, array in tensor_dict[\"buffers\"].items():\n",
    "            module.register_buffer(name, torch.as_tensor(array))    \n",
    "\n",
    "start = perf_counter()\n",
    "replace_tensors(text_encoder_copy, text_encoder_tensors)\n",
    "text_encoder_new = text_encoder_copy.to(\"cuda\")\n",
    "replace_tensors(text_encoder_2_copy, text_encoder_2_tensors)\n",
    "text_encoder_2_new = text_encoder_2_copy.to(\"cuda\")\n",
    "replace_tensors(unet_copy, unet_tensors)\n",
    "unet_new = unet_copy.to(\"cuda\")\n",
    "replace_tensors(vae_copy, vae_tensors)\n",
    "vae_new = vae_copy.to(\"cuda\")\n",
    "endtime = perf_counter()-start\n",
    "print(\"所需时间为: \", endtime, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State_dict 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将 SD-XL 权重加载到系统内存上用时为 0.0871807630173862 s\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "text_encoder_path = \"/data1/workspace/javeyqiu/models/stable-diffusion-xl-base-1.0/text_encoder/model.fp16.safetensors\"\n",
    "text_encoder_2_path = \"/data1/workspace/javeyqiu/models/stable-diffusion-xl-base-1.0/text_encoder_2/model.fp16.safetensors\"\n",
    "unet_model_path = \"/data1/workspace/javeyqiu/models/stable-diffusion-xl-base-1.0/unet/diffusion_pytorch_model.safetensors\"\n",
    "vae_model_path = \"/data1/workspace/javeyqiu/models/huggingface/hub/models--madebyollin--sdxl-vae-fp16-fix/snapshots/207b116dae70ace3637169f1ddd2434b91b3a8cd/diffusion_pytorch_model.safetensors\"\n",
    "text_encoder_weights = preload_safetensors_to_cpu(text_encoder_path)\n",
    "text_encoder_2_weights = preload_safetensors_to_cpu(text_encoder_2_path)\n",
    "unet_weights = preload_safetensors_to_cpu(unet_model_path)\n",
    "vae_weights = preload_safetensors_to_cpu(vae_model_path)\n",
    "endtime = perf_counter()-start_time\n",
    "print(\"将 SD-XL 权重加载到系统内存上用时为\", endtime, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将state_dict从内存中加载到显存上的时间为 0.21457289392128587 s\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "text_encoder.load_state_dict(text_encoder_weights)\n",
    "text_encoder_2.load_state_dict(text_encoder_2_weights)\n",
    "unet.load_state_dict(unet_weights)\n",
    "vae.load_state_dict(vae_weights)\n",
    "endtime = perf_counter()-start\n",
    "print(\"将state_dict从内存中加载到显存上的时间为\", endtime, \"s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd-xl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
